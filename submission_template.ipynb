{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnRX6LUnqBpw"
   },
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part B, Q1 (15 marks)\n",
    "---\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one example. To build models on such data, categorical features have to be encoded or embedded.\n",
    "\n",
    "PyTorch Tabular is a library that makes it very convenient to build neural networks for tabular data. It is built on top of PyTorch Lightning, which abstracts away boilerplate model training code and makes it easy to integrate other tools, e.g. TensorBoard for experiment tracking.\n",
    "\n",
    "For questions B1 and B2, the following features should be used:   \n",
    "- **Numeric / Continuous** features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "- **Categorical** features: month, town, flat_model_type, storey_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Jr6P3U7w3NVl"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (DataConfig, OptimizerConfig, TrainerConfig)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD'] = '1' # https://github.com/suno-ai/bark/issues/626"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGyEWcVlqKTz"
   },
   "source": [
    "> Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data, and year 2021 as test data (validation set is not required).\n",
    "**Do not** use data from year 2022 and year 2023.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "hoCPcOWupw5Y"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df = df[df['year'] < 2022]\n",
    "train_df = df[df['year'] <= 2020]\n",
    "test_df = df[df['year'] == 2021]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sebMgSuzqPe7"
   },
   "source": [
    "> Refer to the documentation of **PyTorch Tabular** and perform the following tasks: https://pytorch-tabular.readthedocs.io/en/latest/#usage\n",
    "- Use **[DataConfig](https://pytorch-tabular.readthedocs.io/en/latest/data/)** to define the target variable, as well as the names of the continuous and categorical variables.\n",
    "- Use **[TrainerConfig](https://pytorch-tabular.readthedocs.io/en/latest/training/)** to automatically tune the learning rate. Set batch_size to be 1024 and set max_epoch as 50.\n",
    "- Use **[CategoryEmbeddingModelConfig](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model)** to create a feedforward neural network with 1 hidden layer containing 50 neurons.\n",
    "- Use **[OptimizerConfig](https://pytorch-tabular.readthedocs.io/en/latest/optimizer/)** to choose Adam optimiser. There is no need to set the learning rate (since it will be tuned automatically) nor scheduler.\n",
    "- Use **[TabularModel](https://pytorch-tabular.readthedocs.io/en/latest/tabular_model/)** to initialise the model and put all the configs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZWAYdNhqPzh"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "data_config = DataConfig(\n",
    "  target=['resale_price'],\n",
    "  continuous_cols=['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm'],\n",
    "  categorical_cols=['month', 'town', 'flat_model_type', 'storey_range']\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "  auto_lr_find=True,\n",
    "  batch_size=1024,\n",
    "  max_epochs=50\n",
    ")\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(\n",
    "  task='regression',\n",
    "  layers='50',\n",
    "  activation='ReLU',\n",
    "  learning_rate=1e-3\n",
    ")\n",
    "\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "  data_config=data_config,\n",
    "  model_config=model_config,\n",
    "  optimizer_config=optimizer_config,\n",
    "  trainer_config=trainer_config\n",
    ")\n",
    "\n",
    "tabular_model.fit(train=train_df, validation=test_df)\n",
    "result = tabular_model.evaluate(test_df)\n",
    "prediction = tabular_model.predict(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UXPKq0qWQG"
   },
   "source": [
    "> Report the test RMSE error and the test R2 value that you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "zmE9Bc7Nqadi"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Value: 0.8474233754422245\n",
      "Root Mean Squared Error: 63529.121763172516\n"
     ]
    }
   ],
   "source": [
    "true_y = test_df['resale_price'].values\n",
    "r2_value = r2_score(true_y, prediction) #type: ignore\n",
    "\n",
    "print(f\"R2 Value: {r2_value}\")\n",
    "print(f\"Root Mean Squared Error: {result[0]['test_loss'] ** 0.5}\") #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJhRU18qX22"
   },
   "source": [
    "> Print out the corresponding rows in the dataframe for the top 25 test samples with the largest errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "5ma5K9vKqZEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        resale_price    prediction         error\n",
      "92405       780000.0  3.801949e+05  399805.06250\n",
      "112128      998000.0  6.727598e+05  325240.18750\n",
      "90251      1001000.0  1.324944e+06  323943.87500\n",
      "90957       968000.0  6.501336e+05  317866.37500\n",
      "91871       680888.0  3.687742e+05  312113.78125\n",
      "90608      1360000.0  1.053320e+06  306679.75000\n",
      "92504       695000.0  3.922965e+05  302703.53125\n",
      "92299       690000.0  3.910424e+05  298957.62500\n",
      "92442      1165000.0  8.684987e+05  296501.31250\n",
      "98379       873000.0  5.839535e+05  289046.50000\n",
      "91694       680000.0  3.917008e+05  288299.21875\n",
      "92340      1245000.0  9.616702e+05  283329.75000\n",
      "92066       628000.0  3.465577e+05  281442.28125\n",
      "93670      1238000.0  9.584582e+05  279541.81250\n",
      "91497       618000.0  3.398550e+05  278145.03125\n",
      "90521       988000.0  7.101542e+05  277845.75000\n",
      "90432      1280000.0  1.005957e+06  274042.87500\n",
      "93825       938000.0  6.652728e+05  272727.25000\n",
      "92073       668000.0  4.001608e+05  267839.15625\n",
      "92496       640000.0  3.742308e+05  265769.15625\n",
      "90523      1260000.0  9.944026e+05  265597.37500\n",
      "92533      1130000.0  8.652801e+05  264719.87500\n",
      "92379       640000.0  3.758652e+05  264134.81250\n",
      "93895       958000.0  6.947778e+05  263222.25000\n",
      "92171       605000.0  3.423907e+05  262609.28125\n"
     ]
    }
   ],
   "source": [
    "errors_df = pd.DataFrame()\n",
    "squeezed_prediction = np.squeeze(prediction) #type: ignore\n",
    "errors_df['resale_price'] = test_df['resale_price']\n",
    "errors_df['prediction'] = squeezed_prediction\n",
    "errors_df['error'] = abs(errors_df['resale_price'] - errors_df['prediction'])\n",
    "print(errors_df.nlargest(25, 'error'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q2 (10 marks)\n",
    "---\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data（validation set is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "train_df = df[df['year'] >= 2021]\n",
    "test_df = df[df['year'] <= 2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 hidden layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 60 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1128/1128 [00:11<00:00, 99.52it/s, loss=2.18e+5, metrics={'r2': -1.7549}] \n",
      "epoch 2: 100%|██████████| 1128/1128 [00:10<00:00, 102.74it/s, loss=1.07e+5, metrics={'r2': 0.4997}]\n",
      "epoch 3: 100%|██████████| 1128/1128 [00:10<00:00, 107.93it/s, loss=9.41e+4, metrics={'r2': 0.6154}]\n",
      "epoch 4: 100%|██████████| 1128/1128 [00:10<00:00, 106.33it/s, loss=8.14e+4, metrics={'r2': 0.7251}]\n",
      "epoch 5: 100%|██████████| 1128/1128 [00:10<00:00, 104.83it/s, loss=7.3e+4, metrics={'r2': 0.7886}] \n",
      "epoch 6: 100%|██████████| 1128/1128 [00:11<00:00, 95.75it/s, loss=6.86e+4, metrics={'r2': 0.817}] \n",
      "epoch 7: 100%|██████████| 1128/1128 [00:12<00:00, 88.87it/s, loss=6.65e+4, metrics={'r2': 0.8307}] \n",
      "epoch 8: 100%|██████████| 1128/1128 [00:11<00:00, 94.43it/s, loss=6.48e+4, metrics={'r2': 0.8406}] \n",
      "epoch 9: 100%|██████████| 1128/1128 [00:12<00:00, 90.66it/s, loss=6.39e+4, metrics={'r2': 0.8452}] \n",
      "epoch 10: 100%|██████████| 1128/1128 [00:11<00:00, 94.73it/s, loss=6.29e+4, metrics={'r2': 0.8511}] \n",
      "epoch 11: 100%|██████████| 1128/1128 [00:12<00:00, 93.04it/s, loss=6.23e+4, metrics={'r2': 0.8542}] \n",
      "epoch 12: 100%|██████████| 1128/1128 [00:12<00:00, 91.34it/s, loss=6.21e+4, metrics={'r2': 0.8556}] \n",
      "epoch 13: 100%|██████████| 1128/1128 [00:10<00:00, 102.69it/s, loss=6.12e+4, metrics={'r2': 0.8598}]\n",
      "epoch 14: 100%|██████████| 1128/1128 [00:11<00:00, 99.54it/s, loss=6.1e+4, metrics={'r2': 0.8607}]  \n",
      "epoch 15: 100%|██████████| 1128/1128 [00:11<00:00, 99.15it/s, loss=6.04e+4, metrics={'r2': 0.8639}] \n",
      "epoch 16: 100%|██████████| 1128/1128 [00:11<00:00, 98.99it/s, loss=6.03e+4, metrics={'r2': 0.8642}] \n",
      "epoch 17: 100%|██████████| 1128/1128 [00:10<00:00, 103.37it/s, loss=5.98e+4, metrics={'r2': 0.8667}]\n",
      "epoch 18: 100%|██████████| 1128/1128 [00:11<00:00, 98.72it/s, loss=5.93e+4, metrics={'r2': 0.8692}] \n",
      "epoch 19: 100%|██████████| 1128/1128 [00:09<00:00, 113.99it/s, loss=5.89e+4, metrics={'r2': 0.8706}]\n",
      "epoch 20: 100%|██████████| 1128/1128 [00:09<00:00, 114.50it/s, loss=5.86e+4, metrics={'r2': 0.8725}]\n",
      "epoch 21: 100%|██████████| 1128/1128 [00:09<00:00, 113.23it/s, loss=5.82e+4, metrics={'r2': 0.8739}]\n",
      "epoch 22: 100%|██████████| 1128/1128 [00:09<00:00, 115.47it/s, loss=5.82e+4, metrics={'r2': 0.8741}]\n",
      "epoch 23: 100%|██████████| 1128/1128 [00:10<00:00, 109.27it/s, loss=5.79e+4, metrics={'r2': 0.8756}]\n",
      "epoch 24: 100%|██████████| 1128/1128 [00:11<00:00, 98.75it/s, loss=5.77e+4, metrics={'r2': 0.8768}] \n",
      "epoch 25: 100%|██████████| 1128/1128 [00:14<00:00, 76.09it/s, loss=5.72e+4, metrics={'r2': 0.8788}] \n",
      "epoch 26: 100%|██████████| 1128/1128 [00:12<00:00, 93.72it/s, loss=5.68e+4, metrics={'r2': 0.8806}] \n",
      "epoch 27: 100%|██████████| 1128/1128 [00:11<00:00, 94.92it/s, loss=5.7e+4, metrics={'r2': 0.8794}] \n",
      "epoch 28: 100%|██████████| 1128/1128 [00:12<00:00, 92.26it/s, loss=5.65e+4, metrics={'r2': 0.8817}] \n",
      "epoch 29: 100%|██████████| 1128/1128 [00:12<00:00, 91.87it/s, loss=5.65e+4, metrics={'r2': 0.8819}]\n",
      "epoch 30: 100%|██████████| 1128/1128 [00:16<00:00, 69.23it/s, loss=5.61e+4, metrics={'r2': 0.8832}] \n",
      "epoch 31: 100%|██████████| 1128/1128 [00:12<00:00, 92.04it/s, loss=5.57e+4, metrics={'r2': 0.8854}] \n",
      "epoch 32: 100%|██████████| 1128/1128 [00:12<00:00, 91.62it/s, loss=5.55e+4, metrics={'r2': 0.8862}] \n",
      "epoch 33: 100%|██████████| 1128/1128 [00:13<00:00, 81.37it/s, loss=5.53e+4, metrics={'r2': 0.8867}]\n",
      "epoch 34: 100%|██████████| 1128/1128 [00:12<00:00, 90.33it/s, loss=5.53e+4, metrics={'r2': 0.8867}]\n",
      "epoch 35: 100%|██████████| 1128/1128 [00:13<00:00, 82.88it/s, loss=5.52e+4, metrics={'r2': 0.8873}] \n",
      "epoch 36: 100%|██████████| 1128/1128 [00:11<00:00, 94.91it/s, loss=5.48e+4, metrics={'r2': 0.889}]  \n",
      "epoch 37: 100%|██████████| 1128/1128 [00:12<00:00, 87.88it/s, loss=5.46e+4, metrics={'r2': 0.8898}] \n",
      "epoch 38: 100%|██████████| 1128/1128 [00:13<00:00, 82.16it/s, loss=5.45e+4, metrics={'r2': 0.8901}]\n",
      "epoch 39: 100%|██████████| 1128/1128 [00:12<00:00, 88.71it/s, loss=5.44e+4, metrics={'r2': 0.8908}]\n",
      "epoch 40: 100%|██████████| 1128/1128 [00:13<00:00, 86.57it/s, loss=5.44e+4, metrics={'r2': 0.8906}]\n",
      "epoch 41: 100%|██████████| 1128/1128 [00:14<00:00, 80.50it/s, loss=5.45e+4, metrics={'r2': 0.8901}]\n",
      "epoch 42: 100%|██████████| 1128/1128 [00:11<00:00, 94.69it/s, loss=5.43e+4, metrics={'r2': 0.8909}] \n",
      "epoch 43: 100%|██████████| 1128/1128 [00:12<00:00, 88.74it/s, loss=5.4e+4, metrics={'r2': 0.892}]   \n",
      "epoch 44: 100%|██████████| 1128/1128 [00:11<00:00, 98.10it/s, loss=5.41e+4, metrics={'r2': 0.8917}] \n",
      "epoch 45: 100%|██████████| 1128/1128 [00:10<00:00, 102.82it/s, loss=5.38e+4, metrics={'r2': 0.8929}]\n",
      "epoch 46: 100%|██████████| 1128/1128 [00:11<00:00, 96.58it/s, loss=5.41e+4, metrics={'r2': 0.8916}] \n",
      "epoch 47: 100%|██████████| 1128/1128 [00:11<00:00, 98.25it/s, loss=5.38e+4, metrics={'r2': 0.8927}] \n",
      "epoch 48: 100%|██████████| 1128/1128 [00:12<00:00, 90.20it/s, loss=5.37e+4, metrics={'r2': 0.8931}] \n",
      "epoch 49: 100%|██████████| 1128/1128 [00:13<00:00, 85.51it/s, loss=5.37e+4, metrics={'r2': 0.8932}] \n",
      "epoch 50: 100%|██████████| 1128/1128 [00:11<00:00, 96.45it/s, loss=5.4e+4, metrics={'r2': 0.8924}]  \n",
      "epoch 51: 100%|██████████| 1128/1128 [00:12<00:00, 90.40it/s, loss=5.35e+4, metrics={'r2': 0.8942}]\n",
      "epoch 52: 100%|██████████| 1128/1128 [00:13<00:00, 85.96it/s, loss=5.38e+4, metrics={'r2': 0.8932}]\n",
      "epoch 53: 100%|██████████| 1128/1128 [00:12<00:00, 86.93it/s, loss=5.32e+4, metrics={'r2': 0.8951}]\n",
      "epoch 54: 100%|██████████| 1128/1128 [00:17<00:00, 63.19it/s, loss=5.35e+4, metrics={'r2': 0.8939}]\n",
      "epoch 55: 100%|██████████| 1128/1128 [00:15<00:00, 71.93it/s, loss=5.35e+4, metrics={'r2': 0.8939}]\n",
      "epoch 56: 100%|██████████| 1128/1128 [00:17<00:00, 62.93it/s, loss=5.34e+4, metrics={'r2': 0.8942}]\n",
      "epoch 57: 100%|██████████| 1128/1128 [00:16<00:00, 68.66it/s, loss=5.34e+4, metrics={'r2': 0.8943}]\n",
      "epoch 58: 100%|██████████| 1128/1128 [00:17<00:00, 65.04it/s, loss=5.34e+4, metrics={'r2': 0.8947}]\n",
      "epoch 59: 100%|██████████| 1128/1128 [00:22<00:00, 49.93it/s, loss=5.31e+4, metrics={'r2': 0.8956}]\n",
      "epoch 60: 100%|██████████| 1128/1128 [00:18<00:00, 59.90it/s, loss=5.33e+4, metrics={'r2': 0.895}] \n"
     ]
    }
   ],
   "source": [
    "target_col = 'resale_price'\n",
    "cat_embed_cols=['month', 'town', 'flat_model_type', 'storey_range']\n",
    "continuous_cols=['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "\n",
    "tab_preprocessor = TabPreprocessor(\n",
    "  cat_embed_cols=cat_embed_cols,\n",
    "  continuous_cols=continuous_cols\n",
    ")\n",
    "X_tab = tab_preprocessor.fit_transform(train_df)\n",
    "target_df = train_df[target_col].values\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "  column_idx=tab_preprocessor.column_idx,\n",
    "  cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "  continuous_cols=tab_preprocessor.continuous_cols,\n",
    "  mlp_hidden_dims=[200, 100]\n",
    ")\n",
    "\n",
    "model = WideDeep(deeptabular=tab_mlp)\n",
    "trainer = Trainer(model, loss_fn='rmse', metrics=[R2Score], num_workers=0)\n",
    "trainer.fit(X_tab=X_tab, target=target_df, n_epochs=60, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1366/1366 [00:04<00:00, 284.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 Score: -0.1337902193666367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "X_tab_te = tab_preprocessor.transform(test_df)\n",
    "predictions = trainer.predict(X_tab=X_tab_te)\n",
    "r2_score = r2_score(test_df[target_col].values, predictions)\n",
    "print(f\"R2 Score: {r2_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q3 (10 marks)\n",
    "---\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, use the train set (year 2020 and before) and test set (year 2021) following the splits in Question B1 (validation set is not required here). To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Standardise the features via **StandardScaler** (fit to training set, then transform all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Input x Gradients, Integrated Gradients, DeepLift, GradientSHAP, Feature Ablation. To avoid long running time, you can limit the analysis to the first 1000 samples in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum to build up your understanding of the difference of various explainability algorithms. Based on your plot, identify the three most important features for regression. Explain how each of these features influences the regression outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q4 (10 marks)\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install alibi-detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Did model degradation occur for the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2020 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOG8ZhA98h3O6fnefkjOU9w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
