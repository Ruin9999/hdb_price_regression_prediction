{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnRX6LUnqBpw"
   },
   "source": [
    "CS4001/4042 Assignment 1\n",
    "---\n",
    "Part B, Q1 (15 marks)\n",
    "---\n",
    "\n",
    "Real world datasets often have a mix of numeric and categorical features – this dataset is one example. To build models on such data, categorical features have to be encoded or embedded.\n",
    "\n",
    "PyTorch Tabular is a library that makes it very convenient to build neural networks for tabular data. It is built on top of PyTorch Lightning, which abstracts away boilerplate model training code and makes it easy to integrate other tools, e.g. TensorBoard for experiment tracking.\n",
    "\n",
    "For questions B1 and B2, the following features should be used:   \n",
    "- **Numeric / Continuous** features: dist_to_nearest_stn, dist_to_dhoby, degree_centrality, eigenvector_centrality, remaining_lease_years, floor_area_sqm\n",
    "- **Categorical** features: month, town, flat_model_type, storey_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "Jr6P3U7w3NVl"
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "from pytorch_tabular import TabularModel\n",
    "from pytorch_tabular.models import CategoryEmbeddingModelConfig\n",
    "from pytorch_tabular.config import (DataConfig, OptimizerConfig, TrainerConfig)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ['TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD'] = '1' # https://github.com/suno-ai/bark/issues/626\n",
    "\n",
    "continuous_features = ['dist_to_nearest_stn', 'dist_to_dhoby', 'degree_centrality', 'eigenvector_centrality', 'remaining_lease_years', 'floor_area_sqm']\n",
    "categorical_features = ['month', 'town', 'flat_model_type', 'storey_range']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGyEWcVlqKTz"
   },
   "source": [
    "> Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from year 2020 and before as training data, and year 2021 as test data (validation set is not required).\n",
    "**Do not** use data from year 2022 and year 2023.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoCPcOWupw5Y"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "train_df = df[df['year'] <= 2020].copy()\n",
    "test_df = df[df['year'] == 2021].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sebMgSuzqPe7"
   },
   "source": [
    "> Refer to the documentation of **PyTorch Tabular** and perform the following tasks: https://pytorch-tabular.readthedocs.io/en/latest/#usage\n",
    "- Use **[DataConfig](https://pytorch-tabular.readthedocs.io/en/latest/data/)** to define the target variable, as well as the names of the continuous and categorical variables.\n",
    "- Use **[TrainerConfig](https://pytorch-tabular.readthedocs.io/en/latest/training/)** to automatically tune the learning rate. Set batch_size to be 1024 and set max_epoch as 50.\n",
    "- Use **[CategoryEmbeddingModelConfig](https://pytorch-tabular.readthedocs.io/en/latest/models/#category-embedding-model)** to create a feedforward neural network with 1 hidden layer containing 50 neurons.\n",
    "- Use **[OptimizerConfig](https://pytorch-tabular.readthedocs.io/en/latest/optimizer/)** to choose Adam optimiser. There is no need to set the learning rate (since it will be tuned automatically) nor scheduler.\n",
    "- Use **[TabularModel](https://pytorch-tabular.readthedocs.io/en/latest/tabular_model/)** to initialise the model and put all the configs together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZZWAYdNhqPzh"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2025</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">02</span>-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">26</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-weight: bold\">16:35:38</span>,<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">086</span> - <span style=\"font-weight: bold\">{</span>pytorch_tabular.tabular_model:<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">146</span><span style=\"font-weight: bold\">}</span> - INFO - Experiment Tracking is turned off           \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;36m2025\u001b[0m-\u001b[1;36m02\u001b[0m-\u001b[1;36m26\u001b[0m \u001b[1;92m16:35:38\u001b[0m,\u001b[1;36m086\u001b[0m - \u001b[1m{\u001b[0mpytorch_tabular.tabular_model:\u001b[1;36m146\u001b[0m\u001b[1m}\u001b[0m - INFO - Experiment Tracking is turned off           \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_config = DataConfig(\n",
    "  target=['resale_price'],\n",
    "  continuous_cols=continuous_features,\n",
    "  categorical_cols=categorical_features,\n",
    ")\n",
    "\n",
    "trainer_config = TrainerConfig(\n",
    "  auto_lr_find=True,\n",
    "  batch_size=1024,\n",
    "  max_epochs=50,\n",
    ")\n",
    "\n",
    "model_config = CategoryEmbeddingModelConfig(task=\"regression\", layers=\"50\")\n",
    "optimizer_config = OptimizerConfig()\n",
    "\n",
    "tabular_model = TabularModel(\n",
    "  data_config=data_config,\n",
    "  model_config=model_config,\n",
    "  optimizer_config=optimizer_config,\n",
    "  trainer_config=trainer_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.5754399373371567\n",
      "Restoring states from the checkpoint path at c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.lr_find_3b35f2b3-53ac-4eff-acfb-fe48b2841958.ckpt\n",
      "Restored all states from the checkpoint at c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.lr_find_3b35f2b3-53ac-4eff-acfb-fe48b2841958.ckpt\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "tabular_model.fit(train=train_df, validation=test_df) #type: ignore\n",
    "result = tabular_model.evaluate(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2UXPKq0qWQG"
   },
   "source": [
    "> Report the test RMSE error and the test R2 value that you obtained.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmE9Bc7Nqadi"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "predictions = tabular_model.predict(test_df)\n",
    "rmse = np.sqrt(mean_squared_error(test_df[\"resale_price\"], predictions))\n",
    "r2 = r2_score(test_df[\"resale_price\"], predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 63529.12561519079\n",
      "Test R2: 0.8474233754422245\n"
     ]
    }
   ],
   "source": [
    "print(\"Test RMSE:\", rmse) #type: ignore\n",
    "print(\"Test R2:\", r2) #type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEJhRU18qX22"
   },
   "source": [
    "> Print out the corresponding rows in the dataframe for the top 25 test samples with the largest errors. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "5ma5K9vKqZEq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        month  year          town                full_address    nearest_stn  \\\n",
      "92405      11  2021   BUKIT MERAH            46 SENG POH ROAD    Tiong Bahru   \n",
      "112128     12  2021      TAMPINES      156 TAMPINES STREET 12       Tampines   \n",
      "90251       4  2021        BISHAN         454 SIN MING AVENUE      Marymount   \n",
      "90957       6  2021   BUKIT BATOK  288A BUKIT BATOK STREET 25    Bukit Batok   \n",
      "91871       6  2021   BUKIT MERAH         17 TIONG BAHRU ROAD    Tiong Bahru   \n",
      "90608      12  2021        BISHAN       273B BISHAN STREET 24         Bishan   \n",
      "92504      12  2021   BUKIT MERAH            49 KIM PONG ROAD    Tiong Bahru   \n",
      "92299      10  2021   BUKIT MERAH         36 MOH GUAN TERRACE    Tiong Bahru   \n",
      "92442      11  2021   BUKIT MERAH          127D KIM TIAN ROAD    Tiong Bahru   \n",
      "98379      12  2021       HOUGANG        615 HOUGANG AVENUE 8        Hougang   \n",
      "91694       4  2021   BUKIT MERAH          35 LIM LIAK STREET    Tiong Bahru   \n",
      "92340      10  2021   BUKIT MERAH            56 HAVELOCK ROAD    Tiong Bahru   \n",
      "92066       8  2021   BUKIT MERAH         48 MOH GUAN TERRACE    Tiong Bahru   \n",
      "93670      12  2021   BUKIT TIMAH              6 TOH YI DRIVE   Beauty World   \n",
      "91497       2  2021   BUKIT MERAH          21 LIM LIAK STREET    Tiong Bahru   \n",
      "90521      10  2021        BISHAN        237 BISHAN STREET 22         Bishan   \n",
      "90432       8  2021        BISHAN       275A BISHAN STREET 24         Bishan   \n",
      "93825       8  2021  CENTRAL AREA       4 TANJONG PAGAR PLAZA  Tanjong Pagar   \n",
      "92073       8  2021   BUKIT MERAH            38 KIM PONG ROAD    Tiong Bahru   \n",
      "92496      12  2021   BUKIT MERAH         48 MOH GUAN TERRACE    Tiong Bahru   \n",
      "90523      10  2021        BISHAN       273B BISHAN STREET 24         Bishan   \n",
      "92533      12  2021   BUKIT MERAH          2C BOON TIONG ROAD    Tiong Bahru   \n",
      "92379      11  2021   BUKIT MERAH         48 MOH GUAN TERRACE    Tiong Bahru   \n",
      "93895      11  2021  CENTRAL AREA       3 TANJONG PAGAR PLAZA  Tanjong Pagar   \n",
      "92171       9  2021   BUKIT MERAH          33 LIM LIAK STREET    Tiong Bahru   \n",
      "\n",
      "        dist_to_nearest_stn  dist_to_dhoby  degree_centrality  \\\n",
      "92405              0.581977       2.309477           0.016807   \n",
      "112128             0.370873      12.479752           0.033613   \n",
      "90251              1.459009       6.840152           0.016807   \n",
      "90957              1.292540      10.763777           0.016807   \n",
      "91871              0.693391       2.058774           0.016807   \n",
      "90608              0.776182       6.297489           0.033613   \n",
      "92504              0.468378       2.365532           0.016807   \n",
      "92299              0.490926       2.278805           0.016807   \n",
      "92442              0.686789       2.664024           0.016807   \n",
      "98379              0.899849       8.828235           0.016807   \n",
      "91694              0.481551       2.262574           0.016807   \n",
      "92340              0.451387       2.128424           0.016807   \n",
      "92066              0.538563       2.345844           0.016807   \n",
      "93670              0.428356       8.948410           0.016807   \n",
      "91497              0.592294       2.146843           0.016807   \n",
      "90521              0.947205       6.663943           0.033613   \n",
      "90432              0.827889       6.370404           0.033613   \n",
      "93825              0.451637       2.594828           0.016807   \n",
      "92073              0.436447       2.323933           0.016807   \n",
      "92496              0.538563       2.345844           0.016807   \n",
      "90523              0.776182       6.297489           0.033613   \n",
      "92533              0.656363       1.982722           0.016807   \n",
      "92379              0.538563       2.345844           0.016807   \n",
      "93895              0.490378       2.630876           0.016807   \n",
      "92171              0.538234       2.217839           0.016807   \n",
      "\n",
      "        eigenvector_centrality        flat_model_type  remaining_lease_years  \\\n",
      "92405                 0.047782       3 ROOM, Standard              50.166667   \n",
      "112128                0.000229  EXECUTIVE, Maisonette              61.750000   \n",
      "90251                 0.013555  EXECUTIVE, Maisonette              67.666667   \n",
      "90957                 0.000217   EXECUTIVE, Apartment              75.583333   \n",
      "91871                 0.047782       3 ROOM, Standard              50.583333   \n",
      "90608                 0.015854           5 ROOM, DBSS              88.833333   \n",
      "92504                 0.047782       3 ROOM, Standard              50.166667   \n",
      "92299                 0.047782       3 ROOM, Standard              50.333333   \n",
      "92442                 0.047782       5 ROOM, Improved              90.333333   \n",
      "98379                 0.001507   EXECUTIVE, Apartment              63.666667   \n",
      "91694                 0.047782       3 ROOM, Standard              50.833333   \n",
      "92340                 0.047782       5 ROOM, Improved              90.750000   \n",
      "92066                 0.047782       3 ROOM, Standard              50.500000   \n",
      "93670                 0.001358  EXECUTIVE, Maisonette              66.666667   \n",
      "91497                 0.047782       3 ROOM, Standard              51.000000   \n",
      "90521                 0.015854       5 ROOM, Improved              69.583333   \n",
      "90432                 0.015854           5 ROOM, DBSS              88.916667   \n",
      "93825                 0.103876  5 ROOM, Adjoined flat              54.583333   \n",
      "92073                 0.047782       3 ROOM, Standard              51.500000   \n",
      "92496                 0.047782       3 ROOM, Standard              50.083333   \n",
      "90523                 0.015854           5 ROOM, DBSS              88.916667   \n",
      "92533                 0.047782       5 ROOM, Improved              78.083333   \n",
      "92379                 0.047782       3 ROOM, Standard              50.250000   \n",
      "93895                 0.103876  5 ROOM, Adjoined flat              54.250000   \n",
      "92171                 0.047782       3 ROOM, Standard              50.416667   \n",
      "\n",
      "        floor_area_sqm storey_range  resale_price  predicted_resale_price  \\\n",
      "92405             88.0     01 TO 03      780000.0            3.801949e+05   \n",
      "112128           148.0     01 TO 03      998000.0            6.727598e+05   \n",
      "90251            243.0     10 TO 12     1001000.0            1.324944e+06   \n",
      "90957            144.0     10 TO 12      968000.0            6.501336e+05   \n",
      "91871             88.0     01 TO 03      680888.0            3.687742e+05   \n",
      "90608            120.0     37 TO 39     1360000.0            1.053320e+06   \n",
      "92504             88.0     01 TO 03      695000.0            3.922965e+05   \n",
      "92299             88.0     01 TO 03      690000.0            3.910424e+05   \n",
      "92442            113.0     16 TO 18     1165000.0            8.684987e+05   \n",
      "98379            142.0     04 TO 06      873000.0            5.839535e+05   \n",
      "91694             88.0     01 TO 03      680000.0            3.917008e+05   \n",
      "92340            114.0     34 TO 36     1245000.0            9.616702e+05   \n",
      "92066             77.0     01 TO 03      628000.0            3.465577e+05   \n",
      "93670            154.0     04 TO 06     1238000.0            9.584582e+05   \n",
      "91497             75.0     01 TO 03      618000.0            3.398550e+05   \n",
      "90521            121.0     07 TO 09      988000.0            7.101542e+05   \n",
      "90432            120.0     25 TO 27     1280000.0            1.005957e+06   \n",
      "93825            118.0     16 TO 18      938000.0            6.652728e+05   \n",
      "92073             88.0     01 TO 03      668000.0            4.001608e+05   \n",
      "92496             77.0     04 TO 06      640000.0            3.742308e+05   \n",
      "90523            120.0     22 TO 24     1260000.0            9.944026e+05   \n",
      "92533            115.0     28 TO 30     1130000.0            8.652801e+05   \n",
      "92379             77.0     04 TO 06      640000.0            3.758652e+05   \n",
      "93895            139.0     07 TO 09      958000.0            6.947778e+05   \n",
      "92171             75.0     01 TO 03      605000.0            3.423907e+05   \n",
      "\n",
      "               error  \n",
      "92405   399805.06250  \n",
      "112128  325240.18750  \n",
      "90251   323943.87500  \n",
      "90957   317866.37500  \n",
      "91871   312113.78125  \n",
      "90608   306679.75000  \n",
      "92504   302703.53125  \n",
      "92299   298957.62500  \n",
      "92442   296501.31250  \n",
      "98379   289046.50000  \n",
      "91694   288299.21875  \n",
      "92340   283329.75000  \n",
      "92066   281442.28125  \n",
      "93670   279541.81250  \n",
      "91497   278145.03125  \n",
      "90521   277845.75000  \n",
      "90432   274042.87500  \n",
      "93825   272727.25000  \n",
      "92073   267839.15625  \n",
      "92496   265769.15625  \n",
      "90523   265597.37500  \n",
      "92533   264719.87500  \n",
      "92379   264134.81250  \n",
      "93895   263222.25000  \n",
      "92171   262609.28125  \n"
     ]
    }
   ],
   "source": [
    "errors_df = test_df.copy()\n",
    "errors_df[\"predicted_resale_price\"] = predictions #type: ignore\n",
    "errors_df[\"error\"] = abs(errors_df[\"resale_price\"] - errors_df[\"predicted_resale_price\"])\n",
    "print(errors_df.sort_values(\"error\", ascending=False).head(25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q2 (10 marks)\n",
    "---\n",
    "In Question B1, we used the Category Embedding model. This creates a feedforward neural network in which the categorical features get learnable embeddings. In this question, we will make use of a library called Pytorch-WideDeep. This library makes it easy to work with multimodal deep-learning problems combining images, text, and tables. We will just be utilizing the deeptabular component of this library through the TabMlp network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pytorch_widedeep import Trainer\n",
    "from pytorch_widedeep.metrics import R2Score\n",
    "from pytorch_widedeep.models import TabMlp, WideDeep\n",
    "from pytorch_widedeep.preprocessing import TabPreprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Divide the dataset (‘hdb_price_prediction.csv’) into train and test sets by using entries from the year 2020 and before as training data, and entries from 2021 and after as the test data（validation set is not required here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "train_df = df[df['year'] <= 2020].copy()\n",
    "test_df = df[df['year'] >= 2021].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Refer to the documentation of Pytorch-WideDeep and perform the following tasks:\n",
    "https://pytorch-widedeep.readthedocs.io/en/latest/index.html\n",
    "* Use [**TabPreprocessor**](https://pytorch-widedeep.readthedocs.io/en/latest/examples/01_preprocessors_and_utils.html#2-tabpreprocessor) to create the deeptabular component using the continuous\n",
    "features and the categorical features. Use this component to transform the training dataset.\n",
    "* Create the [**TabMlp**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/model_components.html#pytorch_widedeep.models.tabular.mlp.tab_mlp.TabMlp) model with 2 hidden layers in the MLP, with 200 and 100 neurons respectively.\n",
    "* Create a [**Trainer**](https://pytorch-widedeep.readthedocs.io/en/latest/pytorch-widedeep/trainer.html#pytorch_widedeep.training.Trainer) for the training of the created TabMlp model with the root mean squared error (RMSE) cost function. Train the model for 60 epochs using this trainer, keeping a batch size of 64. (Note: set the *num_workers* parameter to 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_widedeep\\preprocessing\\tab_preprocessor.py:364: UserWarning: Continuous columns will not be normalised\n",
      "  warnings.warn(\"Continuous columns will not be normalised\")\n",
      "epoch 1: 100%|██████████| 1366/1366 [00:17<00:00, 78.04it/s, loss=1.83e+5] \n",
      "epoch 2: 100%|██████████| 1366/1366 [00:18<00:00, 75.17it/s, loss=9.8e+4]  \n",
      "epoch 3: 100%|██████████| 1366/1366 [00:19<00:00, 70.61it/s, loss=7.78e+4] \n",
      "epoch 4: 100%|██████████| 1366/1366 [00:17<00:00, 76.12it/s, loss=6.61e+4] \n",
      "epoch 5: 100%|██████████| 1366/1366 [00:19<00:00, 69.78it/s, loss=6.14e+4] \n",
      "epoch 6: 100%|██████████| 1366/1366 [00:17<00:00, 76.00it/s, loss=5.97e+4] \n",
      "epoch 7: 100%|██████████| 1366/1366 [00:20<00:00, 65.12it/s, loss=5.85e+4] \n",
      "epoch 8: 100%|██████████| 1366/1366 [00:15<00:00, 90.09it/s, loss=5.75e+4] \n",
      "epoch 9: 100%|██████████| 1366/1366 [00:15<00:00, 90.10it/s, loss=5.67e+4] \n",
      "epoch 10: 100%|██████████| 1366/1366 [00:15<00:00, 91.03it/s, loss=5.61e+4] \n",
      "epoch 11: 100%|██████████| 1366/1366 [00:17<00:00, 79.61it/s, loss=5.52e+4]\n",
      "epoch 12: 100%|██████████| 1366/1366 [00:18<00:00, 72.78it/s, loss=5.47e+4]\n",
      "epoch 13: 100%|██████████| 1366/1366 [00:16<00:00, 85.33it/s, loss=5.41e+4] \n",
      "epoch 14: 100%|██████████| 1366/1366 [00:15<00:00, 86.48it/s, loss=5.35e+4]\n",
      "epoch 15: 100%|██████████| 1366/1366 [00:18<00:00, 74.91it/s, loss=5.3e+4] \n",
      "epoch 16: 100%|██████████| 1366/1366 [00:16<00:00, 84.90it/s, loss=5.24e+4] \n",
      "epoch 17: 100%|██████████| 1366/1366 [00:14<00:00, 94.49it/s, loss=5.19e+4] \n",
      "epoch 18: 100%|██████████| 1366/1366 [00:14<00:00, 92.38it/s, loss=5.15e+4] \n",
      "epoch 19: 100%|██████████| 1366/1366 [00:13<00:00, 99.70it/s, loss=5.1e+4]  \n",
      "epoch 20: 100%|██████████| 1366/1366 [00:13<00:00, 97.97it/s, loss=5.07e+4] \n",
      "epoch 21: 100%|██████████| 1366/1366 [00:16<00:00, 82.14it/s, loss=5.03e+4]\n",
      "epoch 22: 100%|██████████| 1366/1366 [00:17<00:00, 77.99it/s, loss=5e+4]    \n",
      "epoch 23: 100%|██████████| 1366/1366 [00:15<00:00, 90.78it/s, loss=4.97e+4] \n",
      "epoch 24: 100%|██████████| 1366/1366 [00:15<00:00, 87.38it/s, loss=4.94e+4] \n",
      "epoch 25: 100%|██████████| 1366/1366 [00:17<00:00, 79.50it/s, loss=4.93e+4]\n",
      "epoch 26: 100%|██████████| 1366/1366 [00:19<00:00, 69.09it/s, loss=4.92e+4]\n",
      "epoch 27: 100%|██████████| 1366/1366 [00:18<00:00, 72.12it/s, loss=4.92e+4]\n",
      "epoch 28: 100%|██████████| 1366/1366 [00:17<00:00, 79.94it/s, loss=4.89e+4] \n",
      "epoch 29: 100%|██████████| 1366/1366 [00:15<00:00, 89.98it/s, loss=4.86e+4] \n",
      "epoch 30: 100%|██████████| 1366/1366 [00:14<00:00, 91.65it/s, loss=4.87e+4] \n",
      "epoch 31: 100%|██████████| 1366/1366 [00:15<00:00, 90.23it/s, loss=4.83e+4] \n",
      "epoch 32: 100%|██████████| 1366/1366 [00:14<00:00, 96.31it/s, loss=4.82e+4] \n",
      "epoch 33: 100%|██████████| 1366/1366 [00:13<00:00, 97.94it/s, loss=4.83e+4] \n",
      "epoch 34: 100%|██████████| 1366/1366 [00:15<00:00, 90.01it/s, loss=4.8e+4]  \n",
      "epoch 35: 100%|██████████| 1366/1366 [00:16<00:00, 82.78it/s, loss=4.83e+4]\n",
      "epoch 36: 100%|██████████| 1366/1366 [00:15<00:00, 85.47it/s, loss=4.81e+4]\n",
      "epoch 37: 100%|██████████| 1366/1366 [00:15<00:00, 86.44it/s, loss=4.81e+4]\n",
      "epoch 38: 100%|██████████| 1366/1366 [00:28<00:00, 47.68it/s, loss=4.81e+4]\n",
      "epoch 39: 100%|██████████| 1366/1366 [00:31<00:00, 42.79it/s, loss=4.8e+4] \n",
      "epoch 40: 100%|██████████| 1366/1366 [00:19<00:00, 70.25it/s, loss=4.8e+4] \n",
      "epoch 41: 100%|██████████| 1366/1366 [00:23<00:00, 57.64it/s, loss=4.79e+4]\n",
      "epoch 42: 100%|██████████| 1366/1366 [00:31<00:00, 43.91it/s, loss=4.78e+4]\n",
      "epoch 43: 100%|██████████| 1366/1366 [00:43<00:00, 31.73it/s, loss=4.8e+4] \n",
      "epoch 44: 100%|██████████| 1366/1366 [00:42<00:00, 32.31it/s, loss=4.77e+4]\n",
      "epoch 45: 100%|██████████| 1366/1366 [00:42<00:00, 32.10it/s, loss=4.77e+4]\n",
      "epoch 46: 100%|██████████| 1366/1366 [00:23<00:00, 57.41it/s, loss=4.76e+4]\n",
      "epoch 47: 100%|██████████| 1366/1366 [00:25<00:00, 53.97it/s, loss=4.73e+4]\n",
      "epoch 48: 100%|██████████| 1366/1366 [00:16<00:00, 82.01it/s, loss=4.76e+4]\n",
      "epoch 49: 100%|██████████| 1366/1366 [00:19<00:00, 71.72it/s, loss=4.74e+4] \n",
      "epoch 50: 100%|██████████| 1366/1366 [00:15<00:00, 87.25it/s, loss=4.76e+4] \n",
      "epoch 51: 100%|██████████| 1366/1366 [00:16<00:00, 85.27it/s, loss=4.75e+4] \n",
      "epoch 52: 100%|██████████| 1366/1366 [00:26<00:00, 50.71it/s, loss=4.75e+4]\n",
      "epoch 53: 100%|██████████| 1366/1366 [00:25<00:00, 54.24it/s, loss=4.71e+4]\n",
      "epoch 54: 100%|██████████| 1366/1366 [00:26<00:00, 51.01it/s, loss=4.72e+4]\n",
      "epoch 55: 100%|██████████| 1366/1366 [00:21<00:00, 63.30it/s, loss=4.71e+4]\n",
      "epoch 56: 100%|██████████| 1366/1366 [00:18<00:00, 75.37it/s, loss=4.72e+4]\n",
      "epoch 57: 100%|██████████| 1366/1366 [00:22<00:00, 60.00it/s, loss=4.73e+4]\n",
      "epoch 58: 100%|██████████| 1366/1366 [00:20<00:00, 66.75it/s, loss=4.71e+4]\n",
      "epoch 59: 100%|██████████| 1366/1366 [00:22<00:00, 60.00it/s, loss=4.71e+4]\n",
      "epoch 60: 100%|██████████| 1366/1366 [00:20<00:00, 67.52it/s, loss=4.7e+4] \n"
     ]
    }
   ],
   "source": [
    "tab_preprocessor = TabPreprocessor(\n",
    "  cat_embed_cols=categorical_features,\n",
    "  continuous_cols=continuous_features,\n",
    ")\n",
    "tab_x_train = tab_preprocessor.fit_transform(train_df)\n",
    "\n",
    "tab_mlp = TabMlp(\n",
    "  mlp_hidden_dims=[200, 100],\n",
    "  column_idx=tab_preprocessor.column_idx,\n",
    "  cat_embed_input=tab_preprocessor.cat_embed_input,\n",
    "  continuous_cols=continuous_features,\n",
    ")\n",
    "wide_deep = WideDeep(wide=None, deeptabular=tab_mlp)\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=wide_deep,\n",
    "  objective=\"rmse\",\n",
    "  num_workers=0\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "  X_tab=tab_x_train,\n",
    "  target=train_df[\"resale_price\"].values,\n",
    "  n_epochs=60,\n",
    "  batch_size=64,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Report the test RMSE and the test R2 value that you obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict: 100%|██████████| 1128/1128 [00:05<00:00, 209.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 101354.43392913448\n",
      "Test R2: 0.6410912593811802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tab_x_test = tab_preprocessor.transform(test_df)\n",
    "predictions = trainer.predict(X_tab=tab_x_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test_df[\"resale_price\"], predictions))\n",
    "r2 = r2_score(test_df[\"resale_price\"], predictions)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q3 (10 marks)\n",
    "---\n",
    "Besides ensuring that your neural network performs well, it is important to be able to explain the model’s decision. **Captum** is a very handy library that helps you to do so for PyTorch models.\n",
    "\n",
    "Many model explainability algorithms for deep learning models are available in Captum. These algorithms are often used to generate an attribution score for each feature. Features with larger scores are more ‘important’ and some algorithms also provide information about directionality (i.e. a feature with very negative attribution scores means the larger the value of that feature, the lower the value of the output).\n",
    "\n",
    "In general, these algorithms can be grouped into two paradigms:\n",
    "- **perturbation based approaches** (e.g. Feature Ablation)\n",
    "- **gradient / backpropagation based approaches** (e.g. Saliency)\n",
    "\n",
    "The former adopts a brute-force approach of removing / permuting features one by one and does not scale up well. The latter depends on gradients and they can be computed relatively quickly. But unlike how backpropagation computes gradients with respect to weights, gradients here are computed **with respect to the input**. This gives us a sense of how much a change in the input affects the model’s outputs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import optimizer\n",
    "from typing import Tuple, Union, List\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from captum.attr import Saliency, InputXGradient, IntegratedGradients, GradientShap, FeatureAblation, DeepLift\n",
    "\n",
    "NUM_EPOCHS = 200\n",
    "BATCH_SIZE = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> First, use the train set (year 2020 and before) and test set (year 2021) following the splits in Question B1 (validation set is not required here). To keep things simple, we will **limit our analysis to numeric / continuous features only**. Drop all categorical features from the dataframes. Standardise the features via **StandardScaler** (fit to training set, then transform all)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HDBDataset(Dataset):\n",
    "  def __init__(self, x: np.ndarray, y: np.ndarray) -> None:\n",
    "    self.x = torch.from_numpy(x).float()\n",
    "    self.y = torch.from_numpy(y).float()\n",
    "  \n",
    "  def __len__(self) -> int:\n",
    "    return len(self.x)\n",
    "  \n",
    "  def __getitem__(self, index: int) -> Tuple[int, int]:\n",
    "    return self.x[index], self.y[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('hdb_price_prediction.csv')\n",
    "df = df[continuous_features + ['resale_price', 'year']]\n",
    "\n",
    "train_df = df[df['year'] <= 2020].copy()\n",
    "test_df = df[df['year'] == 2021].copy()\n",
    "\n",
    "train_df = train_df.drop(columns=['year'])\n",
    "test_df = test_df.drop(columns=['year'])\n",
    "\n",
    "train_x = train_df[continuous_features]\n",
    "train_y = train_df['resale_price']\n",
    "\n",
    "test_x = test_df[continuous_features]\n",
    "test_y = test_df['resale_price']\n",
    "\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "test_x = scaler.transform(test_x)\n",
    "\n",
    "train_dataset = HDBDataset(train_x, train_y.values)\n",
    "test_dataset = HDBDataset(test_x, test_y.values)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Follow this tutorial to generate the plot from various model explainability algorithms (https://captum.ai/tutorials/House_Prices_Regression_Interpret).\n",
    "Specifically, make the following changes:\n",
    "- Use a feedforward neural network with 3 hidden layers, each having 5 neurons. Train using Adam optimiser with learning rate of 0.001.\n",
    "- Use Input x Gradients, Integrated Gradients, DeepLift, GradientSHAP, Feature Ablation. To avoid long running time, you can limit the analysis to the first 1000 samples in test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([50])) that is different to the input size (torch.Size([50, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([20])) that is different to the input size (torch.Size([20, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1]/[200] running accumulative loss across all batches: 958263681022427136.000\n",
      "Epoch [21]/[200] running accumulative loss across all batches: 112476426161094656.000\n",
      "Epoch [41]/[200] running accumulative loss across all batches: 104504185500729344.000\n",
      "Epoch [61]/[200] running accumulative loss across all batches: 104169461009350656.000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 33\u001b[0m\n\u001b[0;32m     30\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(predictions, y)\n\u001b[0;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 33\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     36\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:340\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    331\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    332\u001b[0m     (inputs,)\n\u001b[0;32m    333\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch\u001b[38;5;241m.\u001b[39mTensor, graph\u001b[38;5;241m.\u001b[39mGradientEdge))\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    336\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m ()\n\u001b[0;32m    337\u001b[0m )\n\u001b[0;32m    339\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[1;32m--> 340\u001b[0m grad_tensors_ \u001b[38;5;241m=\u001b[39m \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:220\u001b[0m, in \u001b[0;36m_make_grads\u001b[1;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    218\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, torch\u001b[38;5;241m.\u001b[39mTensor)\n\u001b[0;32m    219\u001b[0m         new_grads\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 220\u001b[0m             \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m         )\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    223\u001b[0m     new_grads\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class FeedForward(nn.Module):\n",
    "  def __init__(self, in_channels: int = 6, hidden_channels: int = 5, out_channels: int = 1) -> None:\n",
    "    super().__init__()\n",
    "    self.linear_1 = nn.Linear(in_channels, hidden_channels)\n",
    "    self.act_fn_1 = nn.ReLU()\n",
    "    self.linear_2 = nn.Linear(hidden_channels, hidden_channels)\n",
    "    self.act_fn_2 = nn.ReLU()\n",
    "    self.linear_3 = nn.Linear(hidden_channels, out_channels)\n",
    "\n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    x = self.linear_1(x)\n",
    "    x = self.act_fn_1(x)\n",
    "\n",
    "    x = self.linear_2(x)\n",
    "    x = self.act_fn_2(x)\n",
    "\n",
    "    x = self.linear_3(x)\n",
    "    return x  \n",
    "  \n",
    "model = FeedForward()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Train this bitch\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "  running_loss = 0.0\n",
    "  for x, y in train_dataloader:\n",
    "    predictions = model(x)\n",
    "    loss = loss_fn(predictions, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    running_loss += loss.item()\n",
    "    optimizer.step()\n",
    "  if epoch % 20 == 0:\n",
    "    print('Epoch [%d]/[%d] running accumulative loss across all batches: %.3f' %(epoch + 1, NUM_EPOCHS, running_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "174345.24330295942\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "outputs = model(torch.from_numpy(test_x).float())\n",
    "err = np.sqrt(mean_squared_error(outputs.detach().numpy(), test_y))\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\_utils\\gradient.py:57: UserWarning: Input Tensor 0 did not already require gradients, required_grads has been set automatically.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:304: UserWarning: Setting forward, backward hooks and attributes on non-linear\n",
      "               activations. The hooks and attributes will be removed\n",
      "            after the attribution is finished\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "A Module ReLU() was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[83], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m gs_attr_test \u001b[38;5;241m=\u001b[39m gs\u001b[38;5;241m.\u001b[39mattribute(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_x[:\u001b[38;5;241m1000\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat(), torch\u001b[38;5;241m.\u001b[39mfrom_numpy(train_x)\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[0;32m     10\u001b[0m fa_attr_test \u001b[38;5;241m=\u001b[39m fa\u001b[38;5;241m.\u001b[39mattribute(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(test_x[:\u001b[38;5;241m1000\u001b[39m])\u001b[38;5;241m.\u001b[39mfloat())\n\u001b[1;32m---> 11\u001b[0m dl_attr_test \u001b[38;5;241m=\u001b[39m \u001b[43mdl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\log\\__init__.py:42\u001b[0m, in \u001b[0;36mlog_usage.<locals>._log_usage.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:330\u001b[0m, in \u001b[0;36mDeepLift.attribute\u001b[1;34m(self, inputs, baselines, target, additional_forward_args, return_convergence_delta, custom_attribution_func)\u001b[0m\n\u001b[0;32m    320\u001b[0m expanded_target \u001b[38;5;241m=\u001b[39m _expand_target(\n\u001b[0;32m    321\u001b[0m     target, \u001b[38;5;241m2\u001b[39m, expansion_type\u001b[38;5;241m=\u001b[39mExpansionTypes\u001b[38;5;241m.\u001b[39mrepeat\n\u001b[0;32m    322\u001b[0m )\n\u001b[0;32m    324\u001b[0m wrapped_forward_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_forward_func(\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m    326\u001b[0m     (inputs, baselines),\n\u001b[0;32m    327\u001b[0m     expanded_target,\n\u001b[0;32m    328\u001b[0m     additional_forward_args,\n\u001b[0;32m    329\u001b[0m )\n\u001b[1;32m--> 330\u001b[0m gradients \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgradient_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_forward_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m custom_attribution_func \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmultiplies_by_inputs:\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\_utils\\gradient.py:119\u001b[0m, in \u001b[0;36mcompute_gradients\u001b[1;34m(forward_fn, inputs, target_ind, additional_forward_args)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m outputs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTarget not provided when necessary, cannot\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m take gradient with respect to multiple outputs.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    116\u001b[0m     )\n\u001b[0;32m    117\u001b[0m     \u001b[38;5;66;03m# torch.unbind(forward_out) is a list of scalar tensor tuples and\u001b[39;00m\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;66;03m# contains batch_size * #steps elements\u001b[39;00m\n\u001b[1;32m--> 119\u001b[0m     grads \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m grads\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:496\u001b[0m, in \u001b[0;36mgrad\u001b[1;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused, is_grads_batched, materialize_grads)\u001b[0m\n\u001b[0;32m    492\u001b[0m     result \u001b[38;5;241m=\u001b[39m _vmap_internals\u001b[38;5;241m.\u001b[39m_vmap(vjp, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, allow_none_pass_through\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)(\n\u001b[0;32m    493\u001b[0m         grad_outputs_\n\u001b[0;32m    494\u001b[0m     )\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 496\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_outputs_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_unused\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialize_grads:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    507\u001b[0m         result[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor_like(inputs[i])\n\u001b[0;32m    508\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(inputs))\n\u001b[0;32m    509\u001b[0m     ):\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\_utils\\common.py:799\u001b[0m, in \u001b[0;36m_register_backward_hook.<locals>.pre_hook.<locals>.input_tensor_hook\u001b[1;34m(input_grad)\u001b[0m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(grad_out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    798\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 799\u001b[0m hook_out \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_grad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m hook_out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(hook_out, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m hook_out\n",
      "File \u001b[1;32mc:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\captum\\attr\\_core\\deep_lift.py:434\u001b[0m, in \u001b[0;36mDeepLift._backward_hook\u001b[1;34m(self, module, grad_input, grad_output)\u001b[0m\n\u001b[0;32m    432\u001b[0m attr_criteria \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msatisfies_attribute_criteria(module)\n\u001b[0;32m    433\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m attr_criteria:\n\u001b[1;32m--> 434\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    435\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA Module \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m was detected that does not contain some of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    436\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe input/output attributes that are required for DeepLift \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    437\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcomputations. This can occur, for example, if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    438\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour module is being used more than once in the network.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    439\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease, ensure that module is being used only once in the \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    440\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnetwork.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(module)\n\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    443\u001b[0m multipliers \u001b[38;5;241m=\u001b[39m SUPPORTED_NON_LINEAR[\u001b[38;5;28mtype\u001b[39m(module)](\n\u001b[0;32m    444\u001b[0m     module,\n\u001b[0;32m    445\u001b[0m     module\u001b[38;5;241m.\u001b[39minput,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m     eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps,\n\u001b[0;32m    450\u001b[0m )\n\u001b[0;32m    451\u001b[0m \u001b[38;5;66;03m# remove all the properies that we set for the inputs and output\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: A Module ReLU() was detected that does not contain some of the input/output attributes that are required for DeepLift computations. This can occur, for example, if your module is being used more than once in the network.Please, ensure that module is being used only once in the network."
     ]
    }
   ],
   "source": [
    "ixg = InputXGradient(model)\n",
    "ig = IntegratedGradients(model)\n",
    "gs = GradientShap(model)\n",
    "fa = FeatureAblation(model)\n",
    "dl = DeepLift(model)\n",
    "\n",
    "ixg_attr_test = ixg.attribute(torch.from_numpy(test_x[:1000]).float())\n",
    "ig_attr_test = ig.attribute(torch.from_numpy(test_x[:1000]).float(), n_steps=50)\n",
    "gs_attr_test = gs.attribute(torch.from_numpy(test_x[:1000]).float(), torch.from_numpy(train_x).float())\n",
    "fa_attr_test = fa.attribute(torch.from_numpy(test_x[:1000]).float())\n",
    "dl_attr_test = dl.attribute(torch.from_numpy(test_x[:1000]).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Read the following [descriptions](https://captum.ai/docs/attribution_algorithms) and [comparisons](https://captum.ai/docs/algorithms_comparison_matrix) in Captum to build up your understanding of the difference of various explainability algorithms. Based on your plot, identify the three most important features for regression. Explain how each of these features influences the regression outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part B, Q4 (10 marks)\n",
    "---\n",
    "\n",
    "Model degradation is a common issue faced when deploying machine learning models (including neural networks) in the real world. New data points could exhibit a different pattern from older data points due to factors such as changes in government policy or market sentiments. For instance, housing prices in Singapore have been increasing and the Singapore government has introduced 3 rounds of cooling measures over the past years (16 December 2021, 30 September 2022, 27 April 2023).\n",
    "\n",
    "In such situations, the distribution of the new data points could differ from the original data distribution which the models were trained on. Recall that machine learning models often work with the assumption that the test distribution should be similar to train distribution. When this assumption is violated, model performance will be adversely impacted.  In the last part of this assignment, we will investigate to what extent model degradation has occurred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alibi_detect.cd import TabularDrift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2022 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_tabular\\tabular_model.py:1466: DeprecationWarning: `include_input_features` will be deprecated in the next release. Please add index columns to the test dataframe if you want to retain some features like the key or id\n",
      "  warnings.warn(\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_tabular\\categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_tabular\\categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_tabular\\categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n",
      "c:\\Users\\Shun Jie\\Documents\\Github\\hdb_price_regression_prediction\\.venv\\Lib\\site-packages\\pytorch_tabular\\categorical_encoders.py:71: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  X_encoded[col].fillna(self._imputed, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 114670.68872706252\n",
      "Test R2: 0.5463953994310161\n"
     ]
    }
   ],
   "source": [
    "new_test_data = pd.read_csv('hdb_price_prediction.csv')\n",
    "new_test_data = new_test_data[continuous_features + categorical_features + ['resale_price', 'year']]\n",
    "new_test_data = new_test_data[new_test_data['year'] == 2022].copy()\n",
    "new_test_data = new_test_data.drop(columns=['year'])\n",
    "newer_test_data = new_test_data.drop(columns=['resale_price'])\n",
    "\n",
    "predictions = tabular_model.predict(newer_test_data)\n",
    "rmse = np.sqrt(mean_squared_error(new_test_data[\"resale_price\"], predictions))\n",
    "r2 = r2_score(new_test_data[\"resale_price\"], predictions)\n",
    "\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R2:\", r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Evaluate your model from B1 on data from year 2023 and report the test R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Did model degradation occur for the deep learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model degradation could be caused by [various data distribution shifts](https://huyenchip.com/2022/02/07/data-distribution-shifts-and-monitoring.html#data-shift-types): covariate shift (features), label shift and/or concept drift (altered relationship between features and labels).\n",
    "There are various conflicting terminologies in the [literature](https://www.sciencedirect.com/science/article/pii/S0950705122002854#tbl1). Let’s stick to this reference for this assignment.\n",
    "\n",
    "> Using the **Alibi Detect** library, apply the **TabularDrift** function with the training data (year 2020 and before) used as the reference and **detect which features have drifted** in the 2023 test dataset. Before running the statistical tests, ensure you **sample 1000 data points** each from the train and test data. Do not use the whole train/test data. (Hint: use this example as a guide https://docs.seldon.io/projects/alibi-detect/en/stable/examples/cd_chi2ks_adult.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Assuming that the flurry of housing measures have made an impact on the relationship between all the features and resale_price (i.e. P(Y|X) changes), which type of data distribution shift possibly led to model degradation?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From your analysis via TabularDrift, which features contribute to this shift?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Suggest 1 way to address model degradation and implement it, showing improved test R2 for year 2023."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\# TODO: \\<Enter your answer here\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Enter your code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOG8ZhA98h3O6fnefkjOU9w",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
